{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953db153",
   "metadata": {},
   "source": [
    "# DeepScan4Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4ed4f",
   "metadata": {},
   "source": [
    "This is the implementation of an anomaly detection algorithm using Pytorch. It is part of the Data Science Master Thesis of Javier Cuartas, and It was guided by HP-SCDS León 2021-2022, and by Diego García from the University of Cantabria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e0126",
   "metadata": {},
   "source": [
    "Code is structured according to the following tree:\n",
    "    \n",
    "* **train.py** : python file that parse inputs from a shell prompt.\n",
    "* **predict.py** : python file that parse inputs from a shell prompt.\n",
    "\n",
    "\n",
    "* **models** : directory that contains model files (.pt format) and scanner configuration files (.json).\n",
    "* **logs** : directory where all training logs will be stored. \n",
    "* **data** : directory where all input data will be stored.\n",
    "* **docs** : directory where the written part of this work is stored.\n",
    "* **runs** : directory where all tensorboard ouputs will be stored.\n",
    "* **notebooks** : directory where validation jupyter notebooks will be stored.\n",
    "    * **DS4F-Hyperparameter_tunning_example.ipynb**\n",
    "    * **DS4F-Pretreatment-VSB-PowerLineFaultDetection.ipynb**\n",
    "* **docs** : directory where project documents are stored.\n",
    "\n",
    "\n",
    "* **src** : module directory.\n",
    "    * **\\_\\_init\\_\\_.py** : python file that makes this directory a module.\n",
    "    * **dsf4.py** : python file to build scanners.\n",
    "    * **model.py** : python file with the autoencoder models and all related items such as loss function...\n",
    "    * **utils.py** : file with custom performance metrics for the scanner and the dataset modules built with pytorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8b85a",
   "metadata": {},
   "source": [
    "The python module has its own **dsf4.DeepScanner.fit()** and **dsf4.DeepScanner.predict()** methods as you can check printing the corresponding python native functions such as **dir()** or **help()**, but you can also find here an example on how to use the tool from a bash shell with the **train.py** and **predict.py** files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1ca7d",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d969a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--path PATH] [--data_file DATA_FILE]\r\n",
      "                [--training_ids_file TRAINING_IDS_FILE]\r\n",
      "                [--validation_ids_file VALIDATION_IDS_FILE]\r\n",
      "                [--validation_lab_file VALIDATION_LAB_FILE] [--load LOAD]\r\n",
      "                [--save_new SAVE_NEW] [--output_file OUTPUT_FILE]\r\n",
      "                [--scanner_params_file SCANNER_PARAMS_FILE]\r\n",
      "\r\n",
      "Train the model\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --path PATH           Path to the source data files\r\n",
      "  --data_file DATA_FILE\r\n",
      "                        Data file\r\n",
      "  --training_ids_file TRAINING_IDS_FILE\r\n",
      "                        Training ids file name\r\n",
      "  --validation_ids_file VALIDATION_IDS_FILE\r\n",
      "                        Validation ids file name\r\n",
      "  --validation_lab_file VALIDATION_LAB_FILE\r\n",
      "                        Validation labels file name\r\n",
      "  --load LOAD           Model name to load\r\n",
      "  --save_new SAVE_NEW   Trained model name\r\n",
      "  --output_file OUTPUT_FILE\r\n",
      "                        Trained model name\r\n",
      "  --scanner_params_file SCANNER_PARAMS_FILE\r\n",
      "                        File name of the json file containing the desired\r\n",
      "                        scanner parameters in the /models directory, the inner\r\n",
      "                        dictionary key must.\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --path \"/home/ubuntu1/cdir/proj/\" --data_file \"scaled_3.h5\" --training_ids_file \"idtrain_2.csv\" --validation_ids_file \"idvalid_2.csv\" --validation_lab_file \"lbvalid_2.csv\" --save_new True --output_file \"intento_1\" --scanner_params_file \"trainedFromShell.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8095a3",
   "metadata": {},
   "source": [
    "This line takes data from **--data-file** (.h5), splits it into training and validation sets according to **--training_ids_file**, **--validation_ids_file** and **--validation_lab_file** (.csv) from **/data** directory, loads the existing model **--loads** (.pt) from the **/models** directory if It is specified, and trains a new model creating new json files if **--save_new** is set to True, using the **--output-file** name to create any new logs or derived files, and using as well the **--scanner_params_file** json configuration file in the **/models** directory, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1cb6d",
   "metadata": {},
   "source": [
    "**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6307f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py --path \"/home/ubuntu1/cdir/proj/\" --data_file \"scaled_3.h5\" --pr_loss True --ths 0.31 --load \"model_intento_1_20230521_165922.pt\" --config \"intento_1_savedmodels.json\" --output_file \"results_predict.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9632a",
   "metadata": {},
   "source": [
    "**--path** is used for the same purpose in predict.py as it was used in train.py. **--data_file** (.h5) is the file with the data we want to classify into normal/anomaly that must be placed into the **/data** directory, **--pr_loss** saves loss function values in addition to the normal/anomaly classification, **--ths** pass a probability threshold to the predict function so that it can output a binary outcome, **--load** refers to the model which will be used to perform the anomaly detection from the **/models** folder and **--output_file** is the file name of the csv with the input data classified into normal/anomaly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
