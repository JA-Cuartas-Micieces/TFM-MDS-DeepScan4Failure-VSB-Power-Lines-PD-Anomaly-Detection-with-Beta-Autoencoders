{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50961cc",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning of ds4f.DeepScanner for VSB Fault Detection Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87895e4",
   "metadata": {},
   "source": [
    "**License**\n",
    "\n",
    "```MIT License\n",
    "Copyright (c) 2021 HP-SCDS / Observatorio / Máster Data-Science UC /\n",
    "Diego García Saiz / Jesús González Álvarez / Javier Alejandro Cuartas \n",
    "Micieces / 2021-2022 / DeepScan4Failure\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8296c",
   "metadata": {},
   "source": [
    "Hyperparameter tunning was performed on this dataset following these steps:\n",
    "\n",
    "* Splitting into training and validation sets applying k-fold cross-validation in a custom way over the data labeled as normal (semi-supervised approach).\n",
    "\n",
    "\n",
    "* Scaling according to the previously selected training set.\n",
    "\n",
    "\n",
    "* Mean MCC (Mathew's Correlation Coefficient) calculation as the value returned by the objective function to the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475522e8",
   "metadata": {},
   "source": [
    "### ds4f Package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c367e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir)+\"/src\")\n",
    "import ds4f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cc7a5",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Function definition. It generates an 'ename' file with all scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94580420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale(ename,scaledfile,idtrain,idvalidation,mdj,nfeatures,path,dfm):\n",
    "    f_names=[\"h0\",\"lenpksd\",\"pksdov0\",\"pksdbe0\",\"meanprat\",\"stdprat\",\"maxprat\",\"minprat\",\"per01prat\",\"per05prat\",\"per25prat\",\"per50prat\",\"per75prat\",\"per95prat\",\"per99prat\",\"meanpksd\",\"stdpksd\",\"max-minpksd\",\"meanpratr\",\"stdpratr\",\"maxpratr\",\"minpratr\",\"per01pratr\",\"per05pratr\",\"per25pratr\",\"per50pratr\",\"per75pratr\",\"per95pratr\",\"per99pratr\",\"meanpksdr\",\"stdpksdr\",\"max-minpksdr\",\"posx\",\"stdpksdp\"]\n",
    "    f_ext=[c+el for c in [\"per01_\",\"per05_\",\"per25_\",\"per50_\",\"per75_\",\"per95_\",\"per99_\",\"mean_\",\"std_\",\"min_\",\"max_\"] for el in f_names ]\n",
    "    f_ads=[\"lenpx\"]\n",
    "    nwindows=8\n",
    "    full=list()\n",
    "    fullw=[\"wph_\"+str(el) for el in np.arange(nwindows)]\n",
    "    fulla=[\"phase0\",\"target\",\"phase\"]\n",
    "    for nw in range(nwindows+1):\n",
    "      if nw!=nwindows:\n",
    "        full.append([wel+\"_w\"+str(nw) for wel in fullw])\n",
    "      full.append([xt_el+\"_w\"+str(nw) for xt_el in f_ext])\n",
    "      full.append([ds_el+\"_w\"+str(nw) for ds_el in f_ads])\n",
    "    full.append(fulla)\n",
    "    c_names=[f for sl in full for f in sl]\n",
    "\n",
    "    df = pd.DataFrame(columns=c_names)\n",
    "\n",
    "    rws=list()\n",
    "    idlst=idtrain.to_list()+idvalidation.to_list()\n",
    "    for k in np.array(idlst):\n",
    "      with h5py.File(path+ename, \"r\") as f:\n",
    "        dg=f[\"general/\"+str(k)]\n",
    "        rdg=dg[:,:]\n",
    "        rdg=rdg.tolist()[0]\n",
    "      rdg=rdg+[dfm.loc[k,\"target\"],dfm.loc[k,\"phase\"]]\n",
    "      rws.append(rdg)\n",
    "\n",
    "    dfb = pd.DataFrame(rws, columns = c_names,index=idtrain.to_list()+idvalidation.to_list())\n",
    "\n",
    "    ix=['min0','min1','min2','max0','max1','max2','mean0','mean1','mean2','var0','var1','var2']\n",
    "    dfscale = pd.DataFrame(columns=c_names, index=ix)\n",
    "    trainfiltered=dfb.iloc[dfb.index.isin(idtrain.tolist()),:]\n",
    "    for el in ix:\n",
    "      phasefiltered=trainfiltered.loc[trainfiltered.loc[:,\"phase\"]==int(el[-1:]),:]\n",
    "      varv=phasefiltered.min(axis=0) if \"min\" in el else phasefiltered.max(axis=0) if \"max\" in el else phasefiltered.mean(axis=0) if \"mean\" in el else phasefiltered.var(axis=0)\n",
    "      dfscale.loc[el,:] = varv\n",
    "    del phasefiltered,varv\n",
    "\n",
    "    cols=[el for el in dfb.columns if all([\"wph_\" not in el,el not in [\"target\",\"phase\"]])]\n",
    "    negcols=np.concatenate([[\"target\",\"phase\"],[el for el in dfb.columns if \"wph_\" in el]]).tolist()\n",
    "    dfscaled=pd.DataFrame(columns=c_names, index=dfb.index)\n",
    "    for i in range(nfeatures):\n",
    "      refmin=\"min\"+str(i)\n",
    "      refmax=\"max\"+str(i)\n",
    "      dfscaled.loc[dfb.loc[:,\"phase\"]==i,cols]=dfb.loc[dfb.loc[:,\"phase\"]==i,cols].apply(lambda x: (x-dfscale.loc[refmin,cols])/(dfscale.loc[refmax,cols]-dfscale.loc[refmin,cols]), axis=1,result_type='broadcast').astype(np.float64)\n",
    "    dfscaled.loc[:,negcols]=dfb.loc[:,negcols]\n",
    "\n",
    "    fcol=[el for el in dfb.columns if el not in [\"phase\",\"target\"]]\n",
    "    with h5py.File(path+scaledfile, \"w\") as f:\n",
    "      f.create_group(\"general\")\n",
    "      f.flush()\n",
    "\n",
    "    for k in dfscaled.index:\n",
    "      with h5py.File(path+scaledfile, \"a\") as f:\n",
    "        f.create_dataset(\"general/\"+str(k), shape=(1,len(fcol)),dtype=\"f8\")\n",
    "        f.flush()\n",
    "      with h5py.File(path+scaledfile, \"a\") as f:\n",
    "        dg=f[\"general/\"+str(k)]\n",
    "        dg[:,:]=dfscaled.loc[k,fcol]\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe9fa2",
   "metadata": {},
   "source": [
    "## DeepScanner parameter optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98606443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61da43",
   "metadata": {},
   "source": [
    "**GPyOpt Optimization** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387646c",
   "metadata": {},
   "source": [
    "Objective function definition. This will be optimized with GpyOpt to get the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4962ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(params):\n",
    "\n",
    "    global mdj, timestamp,input_size,nrw,optimizer,early_stopping,limlossreadsepochtb,\\\n",
    "                lim_train_size,lim_val_size,epochs,nfeatures,batch_size,nk,\\\n",
    "                path,directorytb,labels_fpth,ename\n",
    "\n",
    "    latent_size=int(params[0][0])\n",
    "    encdec_size=int(params[0][1])\n",
    "    beta=params[0][2]\n",
    "    lr=params[0][3]\n",
    "\n",
    "    dfm=dfm = pd.read_csv(labels_fpth)\n",
    "    hidden_sizes=np.linspace(input_size,latent_size,encdec_size).tolist()\n",
    "    model = {\"input_size\":input_size,\"z_samplesz\":encdec_size,\"hidden_sizes\":[int(i) for i in hidden_sizes]+[latent_size]}\n",
    "    opt_params={\"lr\":lr, \"betas\":(0.9, 0.999), \"weight_decay\":0, \"eps\":1e-8,\"amsgrad\":False, \"maximize\":False, \"foreach\":None,\"capturable\":False}#\"Adam\"\n",
    "\n",
    "    #Random shuffle\n",
    "    idf1=dfm.loc[dfm.loc[:,'target']==1,:]\n",
    "    id1=idf1[\"id_measurement\"].unique().copy()\n",
    "    idf0=dfm.loc[dfm['id_measurement'].apply(lambda x: x not in id1),:]\n",
    "    id0t=idf0['id_measurement'].unique().copy()\n",
    "    np.random.shuffle(id0t)\n",
    "\n",
    "    #K-fold splitting\n",
    "    MCCv=list()\n",
    "    kflen=int(id0t.shape[0]/nk) if (int(id0t.shape[0]/nk)<id1.shape[0]) else id1.shape[0]\n",
    "\n",
    "    for k in range(nk):\n",
    "        id0k=id0t[(k*kflen):((k+1)*kflen)]\n",
    "        id=np.concatenate([id0k,id1])\n",
    "        np.random.shuffle(id)\n",
    "        idv=np.array(pd.Series(id).apply(lambda x: dfm.loc[dfm['id_measurement']==x,:].to_dict(\"records\"))).sum()\n",
    "        idxvalidation=pd.DataFrame(idv).loc[:,['id_measurement','signal_id','target']].reset_index(drop=True)\n",
    "\n",
    "        idtrain=dfm.loc[(dfm[\"signal_id\"].isin(idxvalidation[\"signal_id\"]).tolist()==np.repeat(False,dfm.shape[0])).tolist(),[\"id_measurement\",\"signal_id\"]].reset_index()\n",
    "        idtrain=idtrain.loc[:,\"signal_id\"]\n",
    "        idvalidation=idxvalidation.loc[:,\"signal_id\"]\n",
    "        lbvalidation=idxvalidation.loc[:,\"target\"]\n",
    "        lstfullsig=[144,\n",
    "             145,\n",
    "             146,\n",
    "             1740,\n",
    "             1741,\n",
    "             1742,\n",
    "             2571,\n",
    "             2572,\n",
    "             2573,\n",
    "             3564,\n",
    "             3565,\n",
    "             3566,\n",
    "             5643,\n",
    "             5644,\n",
    "             5645,\n",
    "             6396,\n",
    "             6397,\n",
    "             6398,\n",
    "             7905,\n",
    "             7906,\n",
    "             7907,\n",
    "             8691,\n",
    "             8692,\n",
    "             8693]\n",
    "\n",
    "        idtrain=idtrain.loc[~idtrain.isin(lstfullsig)]\n",
    "        f_valref=idvalidation.isin(lstfullsig)\n",
    "        idvalidation=idvalidation.loc[~f_valref]\n",
    "        lbvalidation=lbvalidation.loc[~f_valref]\n",
    "\n",
    "        filep=\"scaled_\"+str(mdj)+\".h5\"\n",
    "\n",
    "        idtrain=idtrain.reset_index(drop=True)\n",
    "        idvalidation=idvalidation.reset_index(drop=True)\n",
    "        lbvalidation=lbvalidation.reset_index(drop=True)\n",
    "\n",
    "        scale(ename,\"data/\"+filep,idtrain,idvalidation,mdj,nfeatures,path,dfm)\n",
    "\n",
    "        #DeepScanner instance\n",
    "\n",
    "        detector=ds4f.DeepScanner(model,path,\n",
    "                             load=False,savenew=True,cname=str(mdj)+\"trainedScanner\")\n",
    "\n",
    "        #DeepScanner training\n",
    "\n",
    "        detector.fit(filep,idtrain,nfeatures,\n",
    "                     batch_size=batch_size,epochs=epochs,\n",
    "                     beta=beta,early_stopping=early_stopping,\n",
    "                     eval_fn_n=None,\n",
    "                     optimizer=optimizer,optimizer_params=opt_params,\n",
    "                     idvalidation=idvalidation,lbvalidation=lbvalidation,thres_l=np.arange(0.15,0.6,0.001).tolist(),\n",
    "                     lim_val_size=lim_val_size, limlossreadsepochtb=limlossreadsepochtb,\n",
    "                     directorytb=directorytb)\n",
    "\n",
    "        with open('{}savedmodels.json'.format(\"models/\"+str(mdj)+\"trainedScanner_\"), 'r') as f:\n",
    "            g=json.load(f)\n",
    "\n",
    "        with open(detector.modelspath+detector.cname+\"_savedmodels.json\", 'r') as f:\n",
    "            g=json.load(f)\n",
    "            MCCv.append(g[0][list(g[0].keys())[0]][\"v_mccv\"])\n",
    "\n",
    "        del g,detector,idv,idxvalidation,idvalidation,lbvalidation,idtrain\n",
    "        os.remove(path+\"data/\"+filep)\n",
    "        mdj=mdj+1\n",
    "\n",
    "    #Hyperparameter logging.\n",
    "\n",
    "    if os.path.exists(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp)):\n",
    "        logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"a\")\n",
    "    else:\n",
    "        logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"w\")\n",
    "    logft.write(\"**********************************************************************\\n\")\n",
    "    logft.write(\"latent_size:{},encdec_size:{}, beta:{}, lr:{}.\".format(latent_size,encdec_size,beta,lr))\n",
    "    logft.write(\"\\n**********************************************************************\\n\")\n",
    "    logft.write(\"Mean: {}.\".format(np.mean(MCCv)))\n",
    "    logft.write(\"\\nStandard Deviation: {}.\".format(np.std(MCCv)))\n",
    "    logft.write(\"\\n********************************\\n\")\n",
    "    logft.close()\n",
    "\n",
    "    return np.mean(MCCv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ce34b",
   "metadata": {},
   "source": [
    "Hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839dfa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import GPyOpt\n",
    "\n",
    "#Global parameters: Non optimized\n",
    "\n",
    "path=\"/home/ubuntu1/MasterDS-U/\"\n",
    "directorytb='/home/ubuntu1/MasterDS-U/runs/'\n",
    "\n",
    "labels_fpth=path+\"data/metadata_train.csv\"\n",
    "ename=\"data/curatedset.h5\"\n",
    "\n",
    "nk=1\n",
    "nfeatures=3\n",
    "batch_size=8\n",
    "epochs=2\n",
    "lim_val_size=0.5\n",
    "limlossreadsepochtb=1\n",
    "early_stopping=2\n",
    "optimizer = \"Adam\"\n",
    "with h5py.File(path+ename, \"r\") as f:\n",
    "  nr=pd.DataFrame(f[\"general/\"+str(0)][:])\n",
    "nrw=nr.shape[1]\n",
    "input_size=(nfeatures*nrw)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "mdj=0\n",
    "max_time=3600\n",
    "num_acq_points=1\n",
    "max_iter=3\n",
    "\n",
    "#Optimized variables \n",
    "\n",
    "encdec_size_tuple=tuple(np.arange(3,6,2).astype(int))\n",
    "latent_size_tuple=tuple(np.arange(5,1000,1).astype(int))\n",
    "\n",
    "space=[{'name':'latent_size','type':'discrete','domain':latent_size_tuple},\n",
    "       {'name':'encdec_size','type':'discrete','domain':encdec_size_tuple},\n",
    "       {'name':'beta','type':'continuous','domain':(0.5,10)},\n",
    "       {'name':'lr','type':'continuous','domain':(0.00001,0.01)}]\n",
    "\n",
    "optimizerh=GPyOpt.methods.BayesianOptimization(f=objective_function,domain=space, initial_design_numdata=5,num_acq_points=num_acq_points)\n",
    "optimizerh.run_optimization(max_iter=max_iter,max_time=max_time)\n",
    "\n",
    "if os.path.exists(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp)):\n",
    "    logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"a\")\n",
    "else:\n",
    "    logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"w\")\n",
    "logft.write(\"\\n\\n**********************************************************************\\n\")\n",
    "logft.write(\"**********************************************************************\\n\")\n",
    "logft.write(\"BEST SCORE'S HYPERPARAMETERS\\n\")\n",
    "logft.write(\"**********************************************************************\\n\")\n",
    "logft.write(\"latent_size:{},encdec_size:{}, beta:{}, lr:{}.\".format(optimizerh.x_opt[0],optimizerh.x_opt[1],optimizerh.x_opt[2],optimizerh.x_opt[3]))\n",
    "logft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e13a9",
   "metadata": {},
   "source": [
    "**bayesian-optimization Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8220b2",
   "metadata": {},
   "source": [
    "Objective function definition. This will be optimized with bayesian-optimization library to get the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e194e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(beta,latent_size):\n",
    "\n",
    "    global mdj, timestamp,input_size,nrw,optimizer,early_stopping,limlossreadsepochtb,\\\n",
    "                lim_train_size,lim_val_size,epochs,nfeatures,batch_size,nk,\\\n",
    "                path,directorytb,labels_fpth,ename,encdec_size,lr\n",
    "\n",
    "    latent_size=int(latent_size)\n",
    "    encdec_size=encdec_size\n",
    "    beta=beta\n",
    "    lr=lr\n",
    "\n",
    "    dfm=dfm = pd.read_csv(labels_fpth)\n",
    "    hidden_sizes=np.linspace(input_size,latent_size,encdec_size).tolist()\n",
    "    model = {\"input_size\":input_size,\"z_samplesz\":encdec_size,\"hidden_sizes\":[int(i) for i in hidden_sizes]+[latent_size]}\n",
    "    opt_params={\"lr\":lr, \"betas\":(0.9, 0.999), \"weight_decay\":0, \"eps\":1e-8,\"amsgrad\":False, \"maximize\":False, \"foreach\":None,\"capturable\":False}#\"Adam\"\n",
    "\n",
    "    #Random shuffle\n",
    "    idf1=dfm.loc[dfm.loc[:,'target']==1,:]\n",
    "    id1=idf1[\"id_measurement\"].unique().copy()\n",
    "    idf0=dfm.loc[dfm['id_measurement'].apply(lambda x: x not in id1),:]\n",
    "    id0t=idf0['id_measurement'].unique().copy()\n",
    "    np.random.shuffle(id0t)\n",
    "\n",
    "    #K-fold splitting\n",
    "    MCCv=list()\n",
    "    kflen=int(id0t.shape[0]/nk) if (int(id0t.shape[0]/nk)<id1.shape[0]) else id1.shape[0]\n",
    "\n",
    "    for k in range(nk):\n",
    "        id0k=id0t[(k*kflen):((k+1)*kflen)]\n",
    "        id=np.concatenate([id0k,id1])\n",
    "        np.random.shuffle(id)\n",
    "        idv=np.array(pd.Series(id).apply(lambda x: dfm.loc[dfm['id_measurement']==x,:].to_dict(\"records\"))).sum()\n",
    "        idxvalidation=pd.DataFrame(idv).loc[:,['id_measurement','signal_id','target']].reset_index(drop=True)\n",
    "#         idxvalidation=pd.DataFrame(idv).loc[:,['id_measurement','signal_id','target']].reset_index(drop=True)\n",
    "\n",
    "        idtrain=dfm.loc[(dfm[\"signal_id\"].isin(idxvalidation[\"signal_id\"]).tolist()==np.repeat(False,dfm.shape[0])).tolist(),[\"id_measurement\",\"signal_id\"]].reset_index()\n",
    "        idtrain=idtrain.loc[:,\"signal_id\"]\n",
    "        idvalidation=idxvalidation.loc[:,\"signal_id\"]\n",
    "        lbvalidation=idxvalidation.loc[:,\"target\"]\n",
    "        lstfullsig=[144,\n",
    "             145,\n",
    "             146,\n",
    "             1740,\n",
    "             1741,\n",
    "             1742,\n",
    "             2571,\n",
    "             2572,\n",
    "             2573,\n",
    "             3564,\n",
    "             3565,\n",
    "             3566,\n",
    "             5643,\n",
    "             5644,\n",
    "             5645,\n",
    "             6396,\n",
    "             6397,\n",
    "             6398,\n",
    "             7905,\n",
    "             7906,\n",
    "             7907,\n",
    "             8691,\n",
    "             8692,\n",
    "             8693]\n",
    "\n",
    "        idtrain=idtrain.loc[~idtrain.isin(lstfullsig)]\n",
    "        f_valref=idvalidation.isin(lstfullsig)\n",
    "        idvalidation=idvalidation.loc[~f_valref]\n",
    "        lbvalidation=lbvalidation.loc[~f_valref]\n",
    "\n",
    "        filep=\"scaled_\"+str(mdj)+\".h5\"\n",
    "\n",
    "        idtrain=idtrain.reset_index(drop=True)\n",
    "        idvalidation=idvalidation.reset_index(drop=True)\n",
    "        lbvalidation=lbvalidation.reset_index(drop=True)\n",
    "\n",
    "        scale(ename,\"data/\"+filep,idtrain,idvalidation,mdj,nfeatures,path,dfm)\n",
    "\n",
    "        #DeepScanner instance\n",
    "\n",
    "        detector=ds4f.DeepScanner(model,path,\n",
    "                             load=False,savenew=True,cname=str(mdj)+\"trainedScanner\")\n",
    "\n",
    "        #DeepScanner training\n",
    "\n",
    "        detector.fit(filep,idtrain,nfeatures,\n",
    "                     batch_size=batch_size,epochs=epochs,\n",
    "                     beta=beta,early_stopping=early_stopping,\n",
    "                     eval_fn_n=None,\n",
    "                     optimizer=optimizer,optimizer_params=opt_params,\n",
    "                     idvalidation=idvalidation,lbvalidation=lbvalidation,thres_l=np.arange(0.15,0.6,0.001).tolist(),\n",
    "                     lim_val_size=lim_val_size, limlossreadsepochtb=limlossreadsepochtb,\n",
    "                     directorytb=directorytb)\n",
    "\n",
    "        with open('{}savedmodels.json'.format(\"models/\"+str(mdj)+\"trainedScanner_\"), 'r') as f:\n",
    "            g=json.load(f)\n",
    "\n",
    "        with open(detector.modelspath+detector.cname+\"_savedmodels.json\", 'r') as f:\n",
    "            g=json.load(f)\n",
    "            MCCv.append(g[0][list(g[0].keys())[0]][\"v_mccv\"])\n",
    "\n",
    "        del g,detector,idv,idxvalidation,idvalidation,lbvalidation,idtrain\n",
    "        os.remove(path+\"data/\"+filep)\n",
    "        mdj=mdj+1\n",
    "\n",
    "    #Hyperparameter logging.\n",
    "\n",
    "    if os.path.exists(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp)):\n",
    "        logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"a\")\n",
    "    else:\n",
    "        logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"w\")\n",
    "    logft.write(\"**********************************************************************\\n\")\n",
    "    logft.write(\"latent_size:{},encdec_size:{}, beta:{}, lr:{}.\".format(latent_size,encdec_size,beta,lr))\n",
    "    logft.write(\"\\n**********************************************************************\\n\")\n",
    "    logft.write(\"Mean: {}.\".format(np.mean(MCCv)))\n",
    "    logft.write(\"\\nStandard Deviation: {}.\".format(np.std(MCCv)))\n",
    "    logft.write(\"\\n********************************\\n\")\n",
    "    logft.close()\n",
    "\n",
    "    return np.mean(MCCv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e10e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "#Global parameters: Non optimized\n",
    "\n",
    "path=\"/home/ubuntu1/cdir/\"\n",
    "directorytb='/home/ubuntu1/cdir/runs/'\n",
    "\n",
    "labels_fpth=path+\"data/metadata_train.csv\"\n",
    "ename=\"data/curatedset.h5\"\n",
    "\n",
    "nk=1\n",
    "nfeatures=3\n",
    "batch_size=8\n",
    "epochs=200\n",
    "lim_val_size=0.5\n",
    "limlossreadsepochtb=4\n",
    "early_stopping=2\n",
    "optimizer = \"Adam\"\n",
    "lr=0.0005\n",
    "encdec_size=3\n",
    "\n",
    "with h5py.File(path+ename, \"r\") as f:\n",
    "  nr=pd.DataFrame(f[\"general/\"+str(0)][:])\n",
    "nrw=nr.shape[1]\n",
    "input_size=(nfeatures*nrw)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "mdj=0\n",
    "max_time=3600\n",
    "max_iter=3\n",
    "\n",
    "#Optimized variables \n",
    "\n",
    "space={'latent_size':(20,250),\n",
    "       'beta':(1,10)}\n",
    "\n",
    "logger=JSONLogger(path='./logs.log')\n",
    "optimizerh=BayesianOptimization(f=objective_function,pbounds=space)\n",
    "optimizerh.subscribe(Events.OPTIMIZATION_STEP,logger)\n",
    "optimizerh.maximize(n_iter=max_iter,init_points=5)\n",
    "\n",
    "if os.path.exists(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp)):\n",
    "    logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"a\")\n",
    "else:\n",
    "    logft = open(path+\"logs/_hyperparametertunning_{}.log\".format(timestamp), \"w\")\n",
    "logft.write(\"\\n\\n**********************************************************************\\n\")\n",
    "logft.write(\"**********************************************************************\\n\")\n",
    "logft.write(\"BEST SCORE'S HYPERPARAMETERS\\n\")\n",
    "logft.write(\"**********************************************************************\\n\")\n",
    "logft.write(\"latent_size:{},beta:{}.\".format(optimizerh.max['params']['latent_size'],optimizerh.max['params']['beta']))\n",
    "logft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3482119",
   "metadata": {},
   "source": [
    "### Resultados en Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ddaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"/home/ubuntu1/cdir/runs\"\n",
    "!tensorboard dev upload \\\n",
    "  --logdir \"/home/ubuntu1/cdir/runs\" \\\n",
    "  --name \"DL4_1\" \\\n",
    "  --description \"first_t\" \\\n",
    "  --one_shot\n",
    "\n",
    "#Ver 127.0.0.1:6006 para mejor referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89afc6e7",
   "metadata": {},
   "source": [
    "# Final Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaac66b",
   "metadata": {},
   "source": [
    " Training with the full training dataset from the VSB Fault Detection Challenge in Kaggle and use of _train.py_ and _predict.py_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --path \"/home/ubuntu1/cdir/proj/\" \\\n",
    "                --data_file \"scaled_2.h5\" \\\n",
    "                --training_ids_file \"idtrain_2.csv\" \\\n",
    "                --validation_ids_file \"idvalid_2.csv\" \\\n",
    "                --validation_lab_file \"lbvalid_2.csv\" \\\n",
    "                --save_new True \\\n",
    "                --output_file \"intento_1\" \\\n",
    "                --scanner_params_file \"trainedFromShell.json\"#Example provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367117",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py --path \"/home/ubuntu1/cdir/proj/\" \\\n",
    "                    --data_file \"scaled_3.h5\" \\\n",
    "                    --pr_loss True --ths 0.31 \\\n",
    "                    --load \"model_intento_1_20230521_165922.pt\" \\\n",
    "                    --config \"intento_1_savedmodels.json\" \\\n",
    "                    --output_file \"results_predict.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdcd4a",
   "metadata": {},
   "source": [
    "**Visualizing results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de32a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7813c6b5",
   "metadata": {},
   "source": [
    "<a name=\"Bibliography\"></a>\n",
    "# __Bibliography__/__Webgraphy__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb77cd7",
   "metadata": {},
   "source": [
    "* Addison H, Dane S, Vantuch T. _Power Line Fault Detection_ (2018). Kaggle [Online]. Available: https://www.kaggle.com/competitions/vsb-power-line-fault-detection/data.\n",
    "\n",
    "\n",
    "\n",
    "* An J, Cho S. “Variational Autoencoder based Anomaly Detection using Reconstruction Probability”. _Special Lecture on IE_. 2015. Vol. 2. No. 1. pp. 1–18.\n",
    "https://www.semanticscholar.org/paper/Variational-Autoencoder-based-Anomaly-Detection-An-Cho/061146b1d7938d7a8dae70e3531a00fceb3c78e8?p2df [Accesed: Apr. 2023].\n",
    "\n",
    "\n",
    "\n",
    "* Higgins I, Matthey L, Pal A, Burgess C, Glorot X, Botvinick M, Mohamed S, Lerchner A. “beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework”. _International Conference on Learning Representations (ICLR)_. 2017. [Online]. Available:\n",
    "https://openreview.net/forum?id=Sy2fzU9gl [Accessed: Dec. 2021].\n",
    "\n",
    "\n",
    "\n",
    "* Kingma DP, Welling M. “Auto-Encoding Variational Bayes”. arXiv:1312.6114v11 [stat.ML]. Dec 2013. [Online].  Available at:\n",
    "https://arxiv.org/abs/1312.6114 [Accessed: Dec. 2022].\n",
    "\n",
    "\n",
    "\n",
    "* Pytorch, _Pytorch Tutorials_ (2022). Pytorch. Accessed: 2021 Dec 28 [Online]. Available: https://pytorch.org/tutorials/ [Accessed: Apr. 2022].\n",
    "\n",
    "\n",
    "\n",
    "* Pytorch, _Pytorch Documentation_ (2022). Pytorch. Accessed: 2021 Dec 28 [Online]. Available: https://pytorch.org/docs/stable/index.html [Accessed: Apr. 2022].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
