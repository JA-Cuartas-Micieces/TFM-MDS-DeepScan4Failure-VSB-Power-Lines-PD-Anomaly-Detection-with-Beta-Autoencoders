{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL9Q2_aQup4j"
   },
   "source": [
    "# Pretreatment of VSL Fault Detection Problem data for ds4f.DeepScanner validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFUL6ISYeuTO"
   },
   "source": [
    "This is an adaptation of the VSL Fault Detection Dataset for the validation of ds4f.DeepScaner tool, the result of the final thesis DeepScan4Failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9X7DJrTup4m"
   },
   "source": [
    "#### License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVDoZPLGup4n"
   },
   "source": [
    "```MIT License\n",
    "Copyright (c) 2023 / HP-SCDS Observatorio 2021-2022 / Máster Data-Science UC /\n",
    "Diego García Saiz / Javier Alejandro Cuartas Micieces / 2021-2022 / \n",
    "DeepScan4Failure\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHA77uZnup4p"
   },
   "source": [
    "#### Libraries Install and Google Drive Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ARPaWm7CvVA-"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTRU8G8gup4s"
   },
   "source": [
    "<a name=\"Information\"></a>\n",
    "# __Relevant Information and Background__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtXQAlLXn8Gw"
   },
   "source": [
    "Data selected for validation were published by Enet Centre of VSB T.U. Ostrava in a Kaggle competition, several years ago. It is a set of 8.712 time series stored in columns of a parquet format file (800.000 rows of readings each). There is also a csv format file storing indexes and anomaly/non-anomaly labels. Data are voltage signal time series linked to triphasic power lines, so this last csv file is really important to trace each of the time series to groups of 3 that matches to each power line information. ([Enet Centre VSB T.U. Ostrava, 2018](#Bibliography))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GN_Kpuzgup4u"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# datapath=\"/content/gdrive/MyDrive/DeepScan4Failure/data/\"\n",
    "datapath=\"/home/ubuntu1/cdir/data/\"\n",
    "data_dir =datapath\n",
    "datap=os.path.join(datapath,\"train.parquet\")\n",
    "# We load the file with phase groups, anomaly labels, and the foreign key to\n",
    "# each time series (column) in the parquet file.  \n",
    "dfm = pd.read_csv(datapath+\"metadata_train.csv\")\n",
    "meta_train_df = pd.read_csv(data_dir + 'metadata_train.csv')\n",
    "# meta_test_df = pd.read_csv(data_dir + '/metadata_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ep8tnp80TGzA"
   },
   "outputs": [],
   "source": [
    "dfpr=pd.read_parquet(datapath+\"train.parquet\",columns=[\"0\",\"1\",\"2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEi_WxLLJluE"
   },
   "source": [
    "Some rows (series) have been labeled differently as anomalies or normal cases, for each different phase. For now, I have decided to take them as anomalies, labeling them globally as such. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1676961382475,
     "user": {
      "displayName": "Albert Hevirke",
      "userId": "05572075947563068133"
     },
     "user_tz": -60
    },
    "id": "QZr-FgheKKd1",
    "outputId": "0c9de047-1006-4504-d4d3-9e60f1a30727"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8280</th>\n",
       "      <td>8280</td>\n",
       "      <td>2760</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>8281</td>\n",
       "      <td>2760</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8282</th>\n",
       "      <td>8282</td>\n",
       "      <td>2760</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      signal_id  id_measurement  phase  target\n",
       "8280       8280            2760      0       1\n",
       "8281       8281            2760      1       1\n",
       "8282       8282            2760      2       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.loc[dfm[\"id_measurement\"]==2760,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdlbbMrxJ-SU"
   },
   "source": [
    "All measurements addressed through \"id_measurement\" (three phases groups of readings), have 3 \"signal_id\" each. It can be checked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1676961386502,
     "user": {
      "displayName": "Albert Hevirke",
      "userId": "05572075947563068133"
     },
     "user_tz": -60
    },
    "id": "f2eyzJTKJkGU",
    "outputId": "5cad1322-c0f1-4cee-bd4e-7ddad1afcdd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#All measurements addressed through \"id_measurement\" have 3 \"signal_id\" each.\n",
    "np.array([dfm.loc[dfm[\"id_measurement\"]==il,\"signal_id\"].count() for il in dfm[\"id_measurement\"].unique() if dfm.loc[dfm[\"id_measurement\"]==il,\"signal_id\"].count()!=3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vMMmEYRCBZn"
   },
   "source": [
    "There are null values in no phase of no observation, as we can check in the following cell, if we remove the comment character (It takes around 25 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N-irnWR-zJ9X"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# st=time.time()\n",
    "# a=np.array([np.isnan(np.array(pd.read_parquet(datap,columns=[str(el)]))).sum() for el in dfs.columns]).sum() # pandas: reasonable time\n",
    "# tt=time.time()-st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zZj5RfM61xK"
   },
   "source": [
    "<a name=\"Pretreatment\"></a>\n",
    "# __Data Pretreatment__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40eGUsU1IcXl"
   },
   "source": [
    "### Splitting into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjgPx5mqFrzH"
   },
   "source": [
    "Splitting into training and validation set: equal proportion of normal and anomaly observations for the validation set is taken. Then, training set is only made of non-anomalous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vd9GFbH1909m"
   },
   "outputs": [],
   "source": [
    "# In here, anomaly cases (target column equal to 1) are filtered and looped to get\n",
    "# a set with the ids of the size 3 groups that matches anomalous time series and \n",
    "# another set with the ids of randomly selected size 3 groups that matches a\n",
    "# non-anomalous behaviour. \n",
    "\n",
    "idf1=dfm.loc[dfm.loc[:,'target']==1,:]\n",
    "id1=idf1[\"id_measurement\"].unique().copy()\n",
    "idf0=dfm.loc[dfm['id_measurement'].apply(lambda x: x not in id1),:]\n",
    "id0t=idf0['id_measurement'].unique().copy()\n",
    "id0=np.random.choice(id0t, id1.shape[0], replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "g2JoDCrELRop"
   },
   "outputs": [],
   "source": [
    "# Both groups (anomalous and non-anomalous sets) are concatenated and randomly shuffled.\n",
    "\n",
    "id=np.concatenate([id0,id1])\n",
    "np.random.shuffle(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1676961386777,
     "user": {
      "displayName": "Albert Hevirke",
      "userId": "05572075947563068133"
     },
     "user_tz": -60
    },
    "id": "2Mp-yhU7ZJNu",
    "outputId": "d093418a-e0af-4a10-eed0-1725c37cb22d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578</td>\n",
       "      <td>1734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578</td>\n",
       "      <td>1735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578</td>\n",
       "      <td>1736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1594</td>\n",
       "      <td>4782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1594</td>\n",
       "      <td>4783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_measurement  signal_id  target\n",
       "0             578       1734       0\n",
       "1             578       1735       0\n",
       "2             578       1736       0\n",
       "3            1594       4782       0\n",
       "4            1594       4783       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A new index dataframe, similar to the one of the csv, is made of only those observations\n",
    "# which index is in the pandas series of the few last cells.\n",
    "\n",
    "idv=np.array(pd.Series(id).apply(lambda x: dfm.loc[dfm['id_measurement']==x,:].to_dict(\"records\"))).sum()\n",
    "idxvalidation=pd.DataFrame(idv).loc[:,['id_measurement','signal_id','target']].reset_index(drop=True)\n",
    "idxvalidation=pd.DataFrame(idv).loc[:,['id_measurement','signal_id','target']].reset_index(drop=True)\n",
    "idxvalidation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1676961386779,
     "user": {
      "displayName": "Albert Hevirke",
      "userId": "05572075947563068133"
     },
     "user_tz": -60
    },
    "id": "CuedjekI4Xbn",
    "outputId": "9ac2b903-7fa6-4639-b90c-c147a3fcabba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>signal_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id_measurement  signal_id\n",
       "0      0               0          0\n",
       "1      1               0          1\n",
       "2      2               0          2\n",
       "3      6               2          6\n",
       "4      7               2          7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A pandas dataframe is built just like the previous cell's one, but this time with\n",
    "# the training set's 3 phase groups.\n",
    "\n",
    "idtrain=dfm.loc[(dfm[\"signal_id\"].isin(idxvalidation[\"signal_id\"]).tolist()==np.repeat(False,dfm.shape[0])).tolist(),[\"id_measurement\",\"signal_id\"]].reset_index()\n",
    "idtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RR8XVLjiydxd"
   },
   "outputs": [],
   "source": [
    "# Column single phase indexes are filtered from the previous two dataframes so that\n",
    "# they become two pandas series with numbers. There must be a number of consecutive\n",
    "# numbers equal than the number of features (3 in this case according to the 3 phases)\n",
    "# repeated as many times as groups of measures we have for the algorithm to work \n",
    "# properly.\n",
    "\n",
    "idtrain=idtrain.loc[:,\"signal_id\"]\n",
    "idvalidation=idxvalidation.loc[:,\"signal_id\"]\n",
    "\n",
    "# Then, anomaly label series is taken from the validation dataframe.\n",
    "\n",
    "lbvalidation=idxvalidation.loc[:,\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhlWLR5R8Ox3"
   },
   "source": [
    "### Normalizing and reducing the size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c4RTesdpvxd"
   },
   "source": [
    "Just as [Kandanaarachchi et al's (2018)](#Bibliography) work suggests, the best normalization method to apply depends both on the dataset and on the anomaly detection method. Nevertheless, It is min-max scaling that works best with most of the datasets and methods they tried, and that's what it has been aplied here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE0ixiUlwgND"
   },
   "source": [
    "### Pretreatment as PD signal (Partial Discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "95RqaPwpPJBc"
   },
   "outputs": [],
   "source": [
    "path=\"/home/ubuntu1/cdir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "R2e0VBharAcL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: QtAgg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import leastsq\n",
    "import scipy.signal as ssg\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "218-i56JV6pN"
   },
   "source": [
    "#### Functions for pulse extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0Tp-oDYpWAJN"
   },
   "outputs": [],
   "source": [
    "def fit_sinusoid(signal):\n",
    "    \"\"\"\n",
    "    Fits a sinusoid to the signal.\n",
    "    \n",
    "    Source: https://www.kaggle.com/code/jeffreyegan/vsb-power-line-fault-detection-approach\n",
    "    Creator: Jeffrey Egan\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 2*np.pi, len(signal))  # data covers one period\n",
    "    guess_mean = np.mean(signal)\n",
    "    guess_std = 3*np.std(signal)/(2**0.5)/(2**0.5)\n",
    "    guess_phase = 0\n",
    "    guess_freq = 1\n",
    "    guess_amp = 20\n",
    "\n",
    "    # Define the function to optimize, in this case, we want to minimize the difference\n",
    "    # between the actual data and our \"guessed\" parameters\n",
    "    \n",
    "    optimize_func = lambda x: x[0]*np.sin(x[1]*t+x[2]) + x[3] - signal\n",
    "    est_amp, est_freq, est_phase, est_mean = leastsq(optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean])[0]\n",
    "\n",
    "    signal_fit = est_amp*np.sin(est_freq*t+est_phase) + est_mean\n",
    "\n",
    "    return signal_fit\n",
    "\n",
    "def drop_missing(intersect,sample):\n",
    "    \"\"\"\n",
    "    Find intersection of sorted numpy arrays\n",
    "    \n",
    "    Since intersect1d sort arrays each time, it's effectively inefficient.\n",
    "    Here you have to sweep intersection and each sample together to build\n",
    "    the new intersection, which can be done in linear time, maintaining order. \n",
    "\n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    i=j=k=0\n",
    "    new_intersect=np.empty_like(intersect)\n",
    "    while i< intersect.size and j < sample.size:\n",
    "        if intersect[i]==sample[j]: # the 99% case\n",
    "            new_intersect[k]=intersect[i]\n",
    "            k+=1\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif intersect[i]<sample[j]:\n",
    "            i+=1\n",
    "        else : \n",
    "            j+=1\n",
    "    return new_intersect[:k]\n",
    "\n",
    "def clip(v, l, u):\n",
    "    \"\"\"\n",
    "    Numba helper function to clip a value\n",
    "    \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    if v < l:\n",
    "        v = l\n",
    "    elif v > u:\n",
    "        v = u\n",
    "    return v\n",
    "\n",
    "def _local_maxima_1d_window_single_pass(x, w):\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    midpoints = np.empty(x.shape[0] // 2, dtype=np.intp)\n",
    "    left_edges = np.empty(x.shape[0] // 2, dtype=np.intp)\n",
    "    right_edges = np.empty(x.shape[0] // 2, dtype=np.intp)\n",
    "    m = 0  # Pointer to the end of valid area in allocated arrays\n",
    "\n",
    "    i = 1  # Pointer to current sample, first one can't be maxima\n",
    "    i_max = x.shape[0] - 1  # Last sample can't be maxima\n",
    "    while i < i_max:\n",
    "        # Test if previous sample is smaller\n",
    "        if x[i - 1] < x[i]:\n",
    "            i_ahead = i + 1  # Index to look ahead of current sample\n",
    "\n",
    "            # Find next sample that is unequal to x[i]\n",
    "            while i_ahead < i_max and x[i_ahead] == x[i]:\n",
    "                i_ahead += 1\n",
    "                    \n",
    "            i_right = i_ahead - 1\n",
    "            \n",
    "            f = False\n",
    "            i_window_end = i_right + w\n",
    "            while i_ahead < i_max and i_ahead < i_window_end:\n",
    "                if x[i_ahead] > x[i]:\n",
    "                    f = True\n",
    "                    break\n",
    "                i_ahead += 1\n",
    "                \n",
    "            # Maxima is found if next unequal sample is smaller than x[i]\n",
    "            if x[i_ahead] < x[i]:\n",
    "                left_edges[m] = i\n",
    "                right_edges[m] = i_right\n",
    "                midpoints[m] = (left_edges[m] + right_edges[m]) // 2\n",
    "                m += 1\n",
    "                \n",
    "            # Skip samples that can't be maximum\n",
    "            i = i_ahead - 1\n",
    "        i += 1\n",
    "\n",
    "    # Keep only valid part of array memory.\n",
    "    midpoints = midpoints[:m]\n",
    "    left_edges = left_edges[:m]\n",
    "    right_edges = right_edges[:m]\n",
    "    \n",
    "    return midpoints, left_edges, right_edges\n",
    "\n",
    "def local_maxima_1d_window(x, w=1):\n",
    "    \"\"\"\n",
    "    Find local maxima in a 1D array.\n",
    "    This function finds all local maxima in a 1D array and returns the indices\n",
    "    for their midpoints (rounded down for even plateau sizes).\n",
    "    It is a modified version of scipy.signal._peak_finding_utils._local_maxima_1d\n",
    "    to include the use of a window to define how many points on each side to use in\n",
    "    the test for a point being a local maxima.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The array to search for local maxima.\n",
    "    w : np.int\n",
    "        How many points on each side to use for the comparison to be True\n",
    "    Returns\n",
    "    -------\n",
    "    midpoints : ndarray\n",
    "        Indices of midpoints of local maxima in `x`.\n",
    "    Notes\n",
    "    -----\n",
    "    - Compared to `argrelmax` this function is significantly faster and can\n",
    "      detect maxima that are more than one sample wide. However this comes at\n",
    "      the cost of being only applicable to 1D arrays.\n",
    "      \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"    \n",
    "        \n",
    "    fm, fl, fr = _local_maxima_1d_window_single_pass(x, w)\n",
    "    bm, bl, br = _local_maxima_1d_window_single_pass(x[::-1], w)\n",
    "    bm = np.abs(bm - x.shape[0] + 1)[::-1]\n",
    "    bl = np.abs(bl - x.shape[0] + 1)[::-1]\n",
    "    br = np.abs(br - x.shape[0] + 1)[::-1]\n",
    "\n",
    "    m = drop_missing(fm, bm)\n",
    "\n",
    "    return m\n",
    "\n",
    "def plateau_detection(grad, threshold, plateau_length=5):\n",
    "    \"\"\"\n",
    "    Detect the point when the gradient has reach a plateau.\n",
    "    \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    loc = 0\n",
    "    for i in range(grad.shape[0]):\n",
    "        if grad[i] > threshold:\n",
    "            count += 1\n",
    "        \n",
    "        if count == plateau_length:\n",
    "            loc = i - plateau_length\n",
    "            break\n",
    "            \n",
    "    return loc\n",
    "\n",
    "def get_peaks(\n",
    "    x, \n",
    "    window=25):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the peaks in a signal trace.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The array to search.\n",
    "    window : np.int\n",
    "        How many points on each side to use for the local maxima test\n",
    "    Returns\n",
    "    -------\n",
    "    peaks_x : ndarray\n",
    "        Indices of midpoints of peaks in `x`.\n",
    "    peaks_y : ndarray\n",
    "        Absolute heights of peaks in `x`.\n",
    "    x_hp : ndarray\n",
    "        An absolute flattened version of `x`.\n",
    "        \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "\n",
    "    x_hp = x-ssg.savgol_filter(x, 99, 3)\n",
    "    x_dn = np.abs(x_hp)\n",
    "\n",
    "    peaks = local_maxima_1d_window(x_dn, window)\n",
    "\n",
    "    heights = x_dn[peaks]\n",
    "\n",
    "    ii = np.argsort(heights)[::-1]\n",
    "\n",
    "    peaks = peaks[ii]\n",
    "    heights = heights[ii]\n",
    "\n",
    "    ky = heights\n",
    "    kx = np.arange(1, heights.shape[0]+1)\n",
    "    \n",
    "    conv_length = 9\n",
    "\n",
    "    grad = np.diff(ky, 1)/np.diff(kx, 1)\n",
    "    grad = np.convolve(grad, np.ones(conv_length)/conv_length) if any(grad) else grad#, mode='valid') \n",
    "    grad = grad[conv_length-1:-conv_length+1]\n",
    "    \n",
    "    knee_x = plateau_detection(grad, -0.01, plateau_length=1000)\n",
    "    knee_x -= conv_length//2\n",
    "    \n",
    "    peaks_x = peaks[:knee_x]\n",
    "    peaks_y = heights[:knee_x]\n",
    "\n",
    "    ii = np.argsort(peaks_x)\n",
    "    peaks_x = peaks_x[ii]\n",
    "    peaks_y = peaks_y[ii]\n",
    "\n",
    "    return peaks_x, peaks_y, x_hp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6yuLOPPvwub"
   },
   "source": [
    "#### Calculate peak features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "N9uD2Vk0tmIe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_peak_features(px, x_hp0,ruif,wl=25):\n",
    "    \"\"\"\n",
    "    Calculate features for peaks from the provided peaks and signal arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    px : ndarray\n",
    "        Indices of peaks.\n",
    "    x_hp0 : ndarray\n",
    "        The array to search.\n",
    "    ruif : string\n",
    "        Name for the log file.\n",
    "    wl : np.int\n",
    "        How many points on each side to use for large window features\n",
    "    extwdw:\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : ndarray\n",
    "        Features calculate for each peak in `x_hp0`.\n",
    "        \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Changes: Javier Cuartas\n",
    "    \"\"\"\n",
    "    num_peak_features = 34\n",
    "    \n",
    "    features = np.empty((px.shape[0], num_peak_features), dtype=np.float64) if px.shape[0]!=0 else np.zeros((1, num_peak_features), dtype=np.float64)\n",
    "    \n",
    "    logft = open(path+\"loads/load{}.log\".format(ruif), \"a\")\n",
    "    logft.write(\"********************************\")\n",
    "    logft.write(\"\\n\")\n",
    "\n",
    "    flagf=0\n",
    "    if px.shape[0]!=0:\n",
    "        \n",
    "      for i in range(px.shape[0]):\n",
    "\n",
    "          feature_number = 0\n",
    "          x = px[i]\n",
    "          h0 = x_hp0[x]\n",
    "          wl_s = clip(x-wl, 0, 800000-1)\n",
    "          wl_e = clip(x+wl, 0, 800000-1)\n",
    "          x_hp_wl0 = x_hp0[wl_s:wl_e+1]*np.sign(h0)\n",
    "          x_hp_wl0_norm = (x_hp_wl0/np.abs(h0))\n",
    "          \n",
    "          sgref=x_hp_wl0_norm[1:len(x_hp_wl0_norm)]-x_hp_wl0_norm[0:len(x_hp_wl0_norm)-1]\n",
    "          sgref[sgref<0]=-1\n",
    "          sgref[sgref>0]=1\n",
    "            \n",
    "          # First, positions where value is constant are calculated.\n",
    "          chk0=sgref.copy()\n",
    "          chk0[sgref==0]=True\n",
    "          chk0[sgref!=0]=False\n",
    "          chki=np.logical_xor(np.logical_and(chk0[1:len(chk0)],chk0[0:len(chk0)-1]),chk0[0:len(chk0)-1])\n",
    "\n",
    "          # Now, positions where trend changes (local maxima/minima) are calculated.\n",
    "          blk=np.abs(sgref[1:len(sgref)]+sgref[0:len(sgref)-1])\n",
    "          blki=blk.copy()\n",
    "          blki[blk==0]=True\n",
    "          blki[blk!=0]=False\n",
    "          blki=np.logical_and(np.logical_not(chk0[1:len(chk0)]),blki)\n",
    "\n",
    "          pksdp=(np.concatenate(np.argwhere(np.logical_or(chki,blki)))+1).tolist()\n",
    "          logft.write(\"============================================================\")\n",
    "          logft.write(\"\\n\")\n",
    "          logft.write(str(pksdp))\n",
    "          logft.write(\"\\n\")\n",
    "          pksd=np.take(x_hp_wl0_norm,pksdp).tolist() if len(pksdp)>0 else []\n",
    "          pksdr=np.take(x_hp_wl0,pksdp).tolist() if len(pksdp)>0 else []\n",
    "          if np.std(pksdp)<0:\n",
    "            break\n",
    "\n",
    "          prat=np.array(pksd[0:len(pksd)-1])-np.array(pksd[1:len(pksd)]) if len(pksd)>1 else np.max(x_hp_wl0)-np.min(x_hp_wl0)\n",
    "          pratr=np.array(pksdr[0:len(pksdr)-1])-np.array(pksdr[1:len(pksdr)]) if len(pksdr)>1 else np.max(x_hp_wl0_norm)-np.min(x_hp_wl0_norm)\n",
    "\n",
    "          #Each peak environment is described by the next set of features:\n",
    "          #Amount of local minimums/maximums in the peak surroundings:\n",
    "          features[i, feature_number] = h0\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = len(pksd)\n",
    "          feature_number += 1\n",
    "            \n",
    "          #Amount of local minimums below zero in the peak surroundings:\n",
    "          features[i, feature_number] = sum(np.array(pksd)<0)\n",
    "          feature_number += 1 \n",
    "        \n",
    "          #Amount of local maximums above zero in the peak surroundings:\n",
    "          features[i, feature_number] = sum(np.array(pksd)>0)\n",
    "          feature_number += 1\n",
    "            \n",
    "          #Ratio between consecutive differences of local minimum and maximum values:\n",
    "          features[i, feature_number] = np.mean(prat)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.std(prat)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = max(prat)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = min(prat)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,1)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,5)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,25)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,50)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,75)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,95)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(prat,99)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.mean(pksd)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.std(pksd)\n",
    "          feature_number += 1\n",
    "        \n",
    "          #Peak Height:\n",
    "          features[i, feature_number] = max(pksd)-min(pksd)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.mean(pratr)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.std(pratr)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.max(pratr)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.min(pratr)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,1)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,5)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,25)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,50)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,75)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,95)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.percentile(pratr,99) \n",
    "          feature_number += 1\n",
    "            \n",
    "          #Mean of the two portions of the signal:\n",
    "          features[i, feature_number] = np.mean(pksdr)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.std(pksdr)\n",
    "          feature_number += 1\n",
    "        \n",
    "          #Peak Height, position and standard deviation of positions\n",
    "          features[i, feature_number] = np.max(x_hp_wl0)-np.min(x_hp_wl0)\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = x\n",
    "          feature_number += 1\n",
    "          features[i, feature_number] = np.std(pksdp)\n",
    "          feature_number += 1\n",
    "          if i == 0:\n",
    "            assert feature_number == num_peak_features\n",
    "      \n",
    "      features=pd.DataFrame(features,columns=f_names)\n",
    "      fresultp1=features.quantile(0.01,axis=0).to_numpy()\n",
    "      fresultp5=features.quantile(0.05,axis=0).to_numpy()\n",
    "      fresultp25=features.quantile(0.25,axis=0).to_numpy()\n",
    "      fresultp50=features.quantile(0.50,axis=0).to_numpy()\n",
    "      fresultp75=features.quantile(0.75,axis=0).to_numpy()\n",
    "      fresultp95=features.quantile(0.95,axis=0).to_numpy()\n",
    "      fresultp99=features.quantile(0.99,axis=0).to_numpy()\n",
    "      fresultm=features.mean(axis=0).to_numpy()\n",
    "      fresults=features.std(axis=0).to_numpy()\n",
    "      fresultmin=features.min(axis=0).to_numpy()\n",
    "      fresultmax=features.max(axis=0).to_numpy()\n",
    "      fresult=np.concatenate([fresultp1, fresultp5,fresultp25,fresultp50,fresultp75,fresultp95,fresultp99,fresultm,fresults,fresultmin,fresultmax,np.array([len(px),flagf])])\n",
    "\n",
    "    else:\n",
    "      flagf=1\n",
    "      i=0\n",
    "      logft.write(\"_no_peaks\")\n",
    "      feature_number=0\n",
    "      h0 =0\n",
    "      x_hp_wl0 = x_hp0*np.sign(h0)\n",
    "      x_hp_wl0_norm = (x_hp_wl0/np.abs(h0))\n",
    "\n",
    "      pksdp=[]\n",
    "      pksdr=[]\n",
    "      pksd=[]\n",
    "      prat=[]\n",
    "      pratr=[]\n",
    "      \n",
    "      feature_number=0\n",
    "      features[i, feature_number] = h0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1 \n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "        \n",
    "      #Ratio between consecutive differences of local minimum and maximum values:\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "    \n",
    "      #Peak Height:\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "        \n",
    "      #Mean of the two portions of the signal:\n",
    "      features[i, feature_number] =  0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = np.max(x_hp_wl0)-np.min(x_hp_wl0)\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = len(x_hp_wl0)/2\n",
    "      feature_number += 1\n",
    "      features[i, feature_number] = 0\n",
    "      feature_number += 1\n",
    "      \n",
    "      fresult=np.concatenate([np.repeat(features,8),np.repeat([0],num_peak_features),np.repeat(features,2),np.array([0,flagf])])\n",
    " \n",
    "    logft.close()\n",
    "    return fresult\n",
    "\n",
    "def process_signal(\n",
    "    data,ruif,wl,\n",
    "    window=25\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Process a signal trace to find the peaks and calculate features for each peak.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        The array to search.\n",
    "    ruif : string\n",
    "        Name for the log file.\n",
    "    wl : np.int\n",
    "        How many points on each side to use for large window features.\n",
    "    window : np.int\n",
    "        How many points on each side to use for the local maxima test.\n",
    "    Returns\n",
    "    -------\n",
    "    f0 : ndarray\n",
    "        Features calculate for each peak in `data`.\n",
    "                \n",
    "    Source: https://stackoverflow.com/questions/46572308/intersection-of-sorted-numpy-arrays\n",
    "    Creator: B. M.\n",
    "    Modifications: Javier Cuartas\n",
    "    \"\"\"\n",
    "    \n",
    "    px0, height0, x_hp0 = get_peaks(\n",
    "        data.astype(float),\n",
    "        window=window)\n",
    "            \n",
    "    f0 = calculate_peak_features(px0,x_hp0,ruif,wl)\n",
    "    \n",
    "    return f0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoIIvrj8vnoZ"
   },
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhIBQhH6vJ2a"
   },
   "source": [
    "### ***Main Curation Loop - Statistics from time series***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z-ax_cgxPCEF"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m   iref\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_parquet(parquetfile,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(j)])\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#First, a sinusoid is fit for the current signal, and its cuts with x axis are found.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m   kref\u001b[38;5;241m=\u001b[39mfit_sinusoid(iref)\n\u001b[1;32m     58\u001b[0m   krefm\u001b[38;5;241m=\u001b[39mkref\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     59\u001b[0m   kdel\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([])\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mfit_sinusoid\u001b[0;34m(signal)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define the function to optimize, in this case, we want to minimize the difference\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# between the actual data and our \"guessed\" parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimize_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m+\u001b[39mx[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m signal\n\u001b[0;32m---> 20\u001b[0m est_amp, est_freq, est_phase, est_mean \u001b[38;5;241m=\u001b[39m leastsq(optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m signal_fit \u001b[38;5;241m=\u001b[39m est_amp\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(est_freq\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m+\u001b[39mest_phase) \u001b[38;5;241m+\u001b[39m est_mean\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signal_fit\n",
      "File \u001b[0;32m~/miniconda3/envs/TFM0/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:426\u001b[0m, in \u001b[0;36mleastsq\u001b[0;34m(func, x0, args, Dfun, full_output, col_deriv, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maxfev \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    425\u001b[0m         maxfev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\u001b[38;5;241m*\u001b[39m(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     retval \u001b[38;5;241m=\u001b[39m _minpack\u001b[38;5;241m.\u001b[39m_lmdif(func, x0, args, full_output, ftol, xtol,\n\u001b[1;32m    427\u001b[0m                              gtol, maxfev, epsfcn, factor, diag)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col_deriv:\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mfit_sinusoid.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m guess_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define the function to optimize, in this case, we want to minimize the difference\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# between the actual data and our \"guessed\" parameters\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m optimize_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m+\u001b[39mx[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m signal\n\u001b[1;32m     20\u001b[0m est_amp, est_freq, est_phase, est_mean \u001b[38;5;241m=\u001b[39m leastsq(optimize_func, [guess_amp, guess_freq, guess_phase, guess_mean])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m signal_fit \u001b[38;5;241m=\u001b[39m est_amp\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(est_freq\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m+\u001b[39mest_phase) \u001b[38;5;241m+\u001b[39m est_mean\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#The following variables store the number of features to get after the loop. There are\n",
    "#34 features per phase, 11 statistics per earch feature, and 3 phases, as well as 2 additional\n",
    "#global features.\n",
    "\n",
    "nwindows=8\n",
    "nphases=8\n",
    "wl=25\n",
    "nfeatures=3\n",
    "nfeatpphase=34\n",
    "nfeatstats=11\n",
    "nfeatglobl=2\n",
    "nfeatureengineered=1+(nwindows+1)*(nfeatpphase*nfeatstats+nfeatglobl)+nphases*nwindows\n",
    " \n",
    "#n is the iteration number, the id that identifies each of the runs we previously mentioned, \n",
    "#and it matches a group of signals stored in a file (a group of features calculated).\n",
    "#The path of the data source and both destination path and destination filename are set below.\n",
    "\n",
    "N=6\n",
    "path=path\n",
    "parquetfile=path+\"data/train.parquet\"\n",
    "   \n",
    "for n in range(N):\n",
    "    \n",
    "    init=n*1500\n",
    "    endt=1500*(n+1) if (n+1)*1500<8712 else 8712\n",
    "    name=str(init)+\"-\"+str(endt)+\"r_train_stnw\"\n",
    "    dname=name\n",
    "\n",
    "    #First, destination files are created (one dataset per signal).\n",
    "\n",
    "    with h5py.File(path+\"data/\"+name+\".h5\", \"w\") as f:\n",
    "      f.create_group(\"general\")\n",
    "      f.flush\n",
    "\n",
    "    with h5py.File(path+\"data/\"+name+\".h5\", \"w\") as f:\n",
    "      for j in np.arange(init,endt):\n",
    "        f.create_dataset(\"general/\"+str(j), shape= (1,nfeatureengineered),dtype=\"f8\")\n",
    "        f.flush\n",
    "\n",
    "    #phid will be used to perform computations grouping by phase.\n",
    "\n",
    "    phid=np.repeat(np.array(list(range(nphases))),pd.read_parquet(parquetfile,columns=[str(0)]).iloc[:,0].shape[0]/nphases)\n",
    "\n",
    "    #Afterwards, we loop over the set of signals.\n",
    "\n",
    "    i=0\n",
    "    ckr=list()\n",
    "    fts=np.array([])\n",
    "    for j in np.arange(init,endt):\n",
    "\n",
    "      flagflat=0\n",
    "      chkt=list()\n",
    "      iref=pd.read_parquet(parquetfile,columns=[str(j)]).iloc[:,0].to_numpy()\n",
    "\n",
    "    #First, a sinusoid is fit for the current signal, and its cuts with x axis are found.\n",
    "\n",
    "      kref=fit_sinusoid(iref)\n",
    "      krefm=kref.copy()\n",
    "      kdel=np.array([])\n",
    "\n",
    "      lf=0\n",
    "      cts=0\n",
    "      d0=0\n",
    "      d1=0\n",
    "      kiref=list()\n",
    "      kuwtref=list()\n",
    "\n",
    "    #A check of whether the signal belongs to the group of the flat signals is done.\n",
    "\n",
    "      check=np.std(iref)<3\n",
    "\n",
    "      if check==True:\n",
    "        ckr.append(j)\n",
    "\n",
    "    #If the signal doesn't belong to the group of flat signals we perform some further calculations\n",
    "    #which are repeated 4 times to be sure about the .\n",
    "\n",
    "      if check==False:\n",
    "        for cts in np.arange(0,4):\n",
    "          \n",
    "    #Part of the sinusoid fit which was used for previous cts indexes, is removed. The rest is kept \n",
    "    #in krefm.\n",
    "    \n",
    "    #valj should keep the positions where there is a change in the sign of the sinusoid fit, \n",
    "    #d0 and d1 are used to keep the part of the fit which is kept and the influence of this in\n",
    "    #valj in the following cycle.\n",
    "    \n",
    "    #The output of this loop will be a list of positions with the sinusoid fit sign changes in\n",
    "    #kuwtref and the sinusoid fit sign changes position in kiref.\n",
    "    \n",
    "          krefm=np.delete(kref.copy(),kdel) if cts>0 else kref.copy()\n",
    "          valj=abs(krefm).argmin()\n",
    "          val=valj+d0+d1\n",
    "          upwt=-1 if all([val>=len(iref)-3,(kref[val-3]-kref[val])>0]) else 1 if all([val>=len(iref)-3,(kref[val-3]-kref[val])<0]) else 0 if all([val>=len(iref)-3,(kref[val-3]-kref[val])==0]) else -1 if (kref[val]-kref[val+3])>0 else 0 if (kref[val]-kref[val+3])==0 else 1 \n",
    "    \n",
    "          if any([(cts==0),(upwt not in kuwtref)]):\n",
    "            kiref.append(val)\n",
    "            kuwtref.append(upwt)\n",
    "            if len(kiref)==2:\n",
    "              break\n",
    "            else:\n",
    "              lf=val-round((1/5)*len(iref)) if max(abs(val-len(iref)),val)==val else 0\n",
    "              rg=len(iref)-1 if max(abs(val-len(iref)),val)==val else val+round((1/5)*len(iref))-1\n",
    "              kdel=np.concatenate([kdel,np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])]) if cts>0 else np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])\n",
    "              d0=len(np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])) if lf==0 else d0\n",
    "          elif cts<5:\n",
    "            lf=val-round((1/5)*len(iref)) if max(abs(val-len(iref)),val)==val else 0\n",
    "            rg=len(iref)-1 if max(abs(val-len(iref)),val)==val else val+round((1/5)*len(iref))-1\n",
    "            d1=len(np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])) if lf==0 else d1\n",
    "            kdel=np.concatenate([kdel,np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])]) if cts>0 else np.concatenate([np.arange(lf,val),np.arange(val,rg+1)])\n",
    "          else:\n",
    "            break\n",
    "\n",
    "    #Then, sign changes are converted to dataframes so phase per observation is known.\n",
    "            \n",
    "        rows=pd.DataFrame({\"x\":kiref,\"s\":kuwtref}).sort_values(by='x', ascending=True).reset_index(drop=True)\n",
    "        del kref, krefm\n",
    "\n",
    "        zero_ph=np.array([rows.loc[0,\"x\"] if rows.loc[0,\"s\"]>0 else rows.loc[1,\"x\"] if rows.loc[1,\"s\"]>0 else None]) if check==False else np.array([len(iref)+1])\n",
    "        ph0=np.concatenate([phid[-rows.loc[0,\"x\"]:],phid[:len(phid)-rows.loc[0,\"x\"]]]) if rows.loc[0,\"x\"]!=0 else phid if check==False else None\n",
    "        ph1=np.concatenate([phid[-rows.loc[1,\"x\"]:],phid[:len(phid)-rows.loc[1,\"x\"]]]) if rows.loc[1,\"x\"]!=0 else phid if check==False else None\n",
    "        phj=(ph0 if rows.loc[0,\"s\"]==1 else ph1 if rows.loc[0,\"s\"]==-1 else \"error\") if check==False else np.repeat(0,len(iref))\n",
    "\n",
    "    #Iterquartile range is calculated so that extreme observations in each signal can be removed.\n",
    "        \n",
    "        Q1=pd.Series(iref).quantile(0.25)\n",
    "        Q3=pd.Series(iref).quantile(0.75)\n",
    "        bnds=(Q3-Q1)*3\n",
    "\n",
    "        cnd=((iref<=Q3+bnds) & (iref>=Q1-bnds)).copy()\n",
    "        iref=iref[cnd]\n",
    "        phj=phj[cnd]\n",
    "        \n",
    "    #Observations are also split into several windows independently of their phase, features are \n",
    "    #calculated for each one and stored into fts.\n",
    "        \n",
    "        wtdif=np.array([0,len(iref)]) if len(iref)%nwindows==0 else np.array([(len(iref)-nwindows*int(len(iref)/nwindows))/2, len(iref)-(len(iref)-nwindows*int(len(iref)/nwindows))/2]) if (len(iref)-nwindows*int(len(iref)/nwindows))%2==0 else np.array([int((len(iref)-nwindows*int(len(iref)/nwindows))/2),len(iref)-int((len(iref)-nwindows*int(len(iref)/nwindows))/2)-1])\n",
    "        wdj=np.repeat(np.array(list(range(nwindows))),int(len(iref)/nwindows))\n",
    "        iref=iref[int(wtdif[0]):int(wtdif[1])]\n",
    "        phj=phj[int(wtdif[0]):int(wtdif[1])]\n",
    "\n",
    "        for ik in range(nwindows):\n",
    "          phr=np.unique(phj[wdj==ik])\n",
    "          idsph=np.zeros(nphases)    \n",
    "          idsph[phr]=1\n",
    "          f0=process_signal(iref[wdj==ik],dname+\".log\",wl,wl)\n",
    "          fts=np.concatenate([fts,idsph,f0])\n",
    "\n",
    "        f0=process_signal(iref,dname+\".log\",wl,wl)\n",
    "        fts=np.concatenate([fts,f0,zero_ph])\n",
    "\n",
    "    #The current set of features are calculated and stored, and the corresponding array is cleaned.\n",
    "\n",
    "        with h5py.File(path+\"data/\"+name+\".h5\", \"a\") as f:\n",
    "          dg=f[\"general/\"+str(j)]\n",
    "          dg[:,:]=fts\n",
    "          f.flush()\n",
    "        fts=np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatureengineered=nfeatureengineered\n",
    "f_names=[\"h0\",\"lenpksd\",\"pksdov0\",\"pksdbe0\",\"meanprat\",\"stdprat\",\"maxprat\",\"minprat\",\"per01prat\",\"per05prat\",\"per25prat\",\"per50prat\",\"per75prat\",\"per95prat\",\"per99prat\",\"meanpksd\",\"stdpksd\",\"max-minpksd\",\"meanpratr\",\"stdpratr\",\"maxpratr\",\"minpratr\",\"per01pratr\",\"per05pratr\",\"per25pratr\",\"per50pratr\",\"per75pratr\",\"per95pratr\",\"per99pratr\",\"meanpksdr\",\"stdpksdr\",\"max-minpksdr\",\"posx\",\"stdpksdp\"]\n",
    "f_ext=[c+el for c in [\"per01_\",\"per05_\",\"per25_\",\"per50_\",\"per75_\",\"per95_\",\"per99_\",\"mean_\",\"std_\",\"min_\",\"max_\"] for el in f_names ]\n",
    "f_ads=[\"lenpx\"]\n",
    "nwindows=8\n",
    "full=list()\n",
    "fullw=[\"wph_\"+str(el) for el in np.arange(nwindows)]\n",
    "fulla=[\"phase0\",\"target\",\"phase\"]\n",
    "for nw in range(nwindows+1):\n",
    "  if nw!=nwindows:\n",
    "    full.append([wel+\"_w\"+str(nw) for wel in fullw])\n",
    "  full.append([xt_el+\"_w\"+str(nw) for xt_el in f_ext])\n",
    "  full.append([ds_el+\"_w\"+str(nw) for ds_el in f_ads])\n",
    "full.append(fulla)\n",
    "c_names=[f for sl in full for f in sl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0JPnzMtlcmf"
   },
   "source": [
    "### ***Joining Preprocessed files***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzwNIJPWGz4M"
   },
   "outputs": [],
   "source": [
    "nfeatures=3\n",
    "path='/home/ubuntu1/cdir/'\n",
    "\n",
    "inif=0\n",
    "endf= meta_train_df.shape[0]\n",
    "step=1500\n",
    "dname=\"data/\"\n",
    "ename=\"data/curatedset_def.h5\"\n",
    "os.chdir(path+dname)\n",
    "k=os.listdir()\n",
    "k.sort()\n",
    "\n",
    "with h5py.File(path+ename, \"w\") as f:\n",
    "  f.create_group(\"general\")\n",
    "  f.flush\n",
    "with h5py.File(path+ename, \"w\") as f:\n",
    "  for j in np.arange(inif,endf):\n",
    "    f.create_dataset(\"general/\"+str(j), shape= (1,nfeatureengineered),dtype=\"f8\")\n",
    "    f.flush\n",
    "\n",
    "i=0\n",
    "for r in k:\n",
    "  print(\"==================\")\n",
    "  initj=i if i==0 else endtj if endtj<endf else None\n",
    "  if initj==None: break\n",
    "  endtj=initj+step if initj+step<endf else endf\n",
    "  i=i+1\n",
    "  for j in np.arange(initj,endtj):\n",
    "    rdg=np.array([])\n",
    "    with h5py.File(path+dname+r, \"r\") as f:\n",
    "      dg=f[\"general/\"+str(j)]\n",
    "      rdg=dg[:,:]\n",
    "    with h5py.File(path+ename, \"a\") as n:\n",
    "      ndg=n[\"general/\"+str(j)]\n",
    "      ndg[:,:]=rdg\n",
    "      n.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Removing flat signals***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "apUdxWX91WHr"
   },
   "outputs": [],
   "source": [
    "lstfullsig=[144,\n",
    " 145,\n",
    " 146,\n",
    " 1740,\n",
    " 1741,\n",
    " 1742,\n",
    " 2571,\n",
    " 2572,\n",
    " 2573,\n",
    " 3564,\n",
    " 3565,\n",
    " 3566,\n",
    " 5643,\n",
    " 5644,\n",
    " 5645,\n",
    " 6396,\n",
    " 6397,\n",
    " 6398,\n",
    " 7905,\n",
    " 7906,\n",
    " 7907,\n",
    " 8691,\n",
    " 8692,\n",
    " 8693]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1676915193679,
     "user": {
      "displayName": "Albert Hevirke",
      "userId": "05572075947563068133"
     },
     "user_tz": -60
    },
    "id": "Sf1trYlmHZI6",
    "outputId": "d0e84a1c-ad9e-40bc-a578-d3e27ac5dbbe"
   },
   "outputs": [],
   "source": [
    "type(idtrain)\n",
    "print(len(idtrain))\n",
    "print(idtrain.loc[~idtrain.isin(lstfullsig)].shape)\n",
    "print(len(idvalidation))\n",
    "print(idvalidation.loc[~idvalidation.isin(lstfullsig)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(idtrain.isin(np.array(lstfullsig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(idtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "npz1uFGsHahK"
   },
   "outputs": [],
   "source": [
    "idtrain=idtrain.loc[~idtrain.isin(lstfullsig)]\n",
    "f_valref=idvalidation.isin(lstfullsig)\n",
    "idvalidation=idvalidation.loc[~f_valref]\n",
    "lbvalidation=lbvalidation.loc[~f_valref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t96gpprJYVqa"
   },
   "source": [
    "<a name=\"Bibliography\"></a>\n",
    "# __Bibliography__/__Webgraphy__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQm9Kc5Z9yAo"
   },
   "source": [
    "* Addison H, Dane S, Vantuch T. _Power Line Fault Detection_ (2018). Kaggle [Online]. Available: https://www.kaggle.com/competitions/vsb-power-line-fault-detection/data. [Accesed: Apr. 2023].\n",
    "\n",
    "\n",
    "* Egan J. _VSB Power Line Fault Detection Approach_. [Competition Notebook]. Kaggle: Power Line Fault Detection Competition of  Enet Centre, VSB – TU of Ostrava. Egan J. 2019. \n",
    "https://www.kaggle.com/code/jeffreyegan/vsb-power-line-fault-detection-approach [Accessed Sep. 2022]. \n",
    "\n",
    "\n",
    "* Mark4h. _VSB_1st_place_solution_. [Competition Notebook]. Kaggle: Power Line Fault Detection Competition of  Enet Centre, VSB – TU of Ostrava. Mark4h. 2019. \n",
    "https://www.kaggle.com/code/mark4h/vsb-1st-place-solution [Accessed Sep. 2022].\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "V9X7DJrTup4m",
    "tHA77uZnup4p",
    "YTRU8G8gup4s",
    "40eGUsU1IcXl",
    "EhlWLR5R8Ox3",
    "Qdd4GibywXs4",
    "fE0ixiUlwgND",
    "bVeCE_gbwLWg",
    "Z6yuLOPPvwub",
    "zoIIvrj8vnoZ",
    "QhIBQhH6vJ2a",
    "gJgpEB-1lRvl",
    "f0JPnzMtlcmf",
    "vMPE32Kw3AJi",
    "pw7Fa4Mg3Gxk",
    "t96gpprJYVqa"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
